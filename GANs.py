
from __future__ import print_function
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.optim as optim
import torch.utils.data
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torchvision.utils as vutils
from torch.autograd import Variable


# Defining the weights_init funciton that takes as input as a Neural Network 'm' and that will initialize all its weights.
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') !=-1:
        m.weight.data.normal_(0.0,0.02)
    elif classname.find('BatchNorm') !=-1:
        m.weight.data.normal_(1.0,0.02)
        m.bias.data.fill_(0)

# Defining the Neural Network Architechture for Generator
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.main = nn.Sequential(
            nn.ConvTranspose2d(in_channels=100, out_channels=512, kernel_size=4, stride=1, padding=0, bias=False),
            nn.BatchNorm2d(num_features=512),
            nn.ReLU(True),
            nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(num_features=256),
            nn.ReLU(True),
            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(num_features=128),
            nn.ReLU(True),
            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),   
            nn.BatchNorm2d(num_features=64),
            nn.ReLU(True),
            nn.ConvTranspose2d(in_channels=64, out_channels=3, kernel_size=4, stride=2, padding=1, bias=False),
            nn.Tanh()
        )
    def forward(self, input):
        output = self.main(input)
        return 
    

# Defining the Neural Network Architechture for Generator
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.main = nn.Sequential(
            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),
            nn.LeakyReLU(negative_slope=0.2, inplace=True),
            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(num_features=128),
            nn.LeakyReLU(negative_slope=0.2, inplace=True),
            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(num_features=256),
            nn.LeakyReLU(negative_slope=0.2, inplace=True),
            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(num_features=512),
            nn.LeakyReLU(negative_slope=0.2, inplace=True),
            nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=0, bias=False),
            nn.Sigmoid()
        )

    def foward(self, input):
        output = self.main(input)
        return output.view(-1)
    


# Main Function 
if __name__ == "__main__":
    # Setting some parameters
    batchSize =64
    imageSize = 64

    # Creating the Transformations
    transform = transforms.Compose([transforms.Resize(imageSize), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])

    # Loading the dataset
    dataset = dset.CIFAR10(root='./data', download=True, transform=transform)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers = 2)
    
    # Creating Generator Object
    netG = Generator()
    netG.apply(weights_init)

    # Creating Discriminator Object
    netD = Discriminator()
    netD.apply(weights_init)

    # Training the DCGANs
    criteria = nn.BCELoss()
    optimizerD = optim.Adam(params=netD.parameters(), lr=0.0002, betas=(0.5, 0.999))
    optimizerG = optim.Adam(params=netG.parameters(), lr=0.0002, betas=(0.5, 0.999))

    for epoch in range(35):
        for i, data in enumerate(dataloader,0):
            # Step 1: Updating the weights of the Neural Network of the Discriminator
            netD.zero_grad()

            # Trainging the Discriminator with the real image of the Dataset
            real,_ = data
            input = Variable(real)
            target = Variable(torch.ones(input.size()[0]))
            output = netD(input)
            errD_real = criteria(output, target)

            # Trainging the Discriminator with a Fake image generated by the Generator
            noise = Variable(torch.randn(input.size()[0], 100, 1, 1))
            fake = netG(noise)
            target = Variable(torch.zeros(input.size()[0]))
            errD_fake = criteria(output, target)
            output = netD(fake.detach())

            # Backpropagating the totat error
            errD_total = errD_real + errD_fake
            errD_total.backward()
            optimizerD.step()

            # Step 2: Updating the weights of the Neural Network of the Genertor
            netG.zero_grad()
            target = Variable(torch.ones(input.size()[0]))
            output = netD(fake)
            errG = criteria(output, target)
            errG.backward()
            optimizerG.step()

            # 3rd Step: Printing the losses and saving the real images and the generated images of the minibatch every 100 steps
            if i % 100 == 0: # Every 100 steps:
                vutils.save_image(real, '%s/real_samples.png' % "./results", normalize = True) # We save the real images of the minibatch.
                fake = netG(noise) # We get our fake generated images.
                vutils.save_image(fake.data, '%s/arun_fake_samples_epoch_%03d.png' % ("./results", epoch), normalize = True) # We also save the fake generated images of the minibatch.
                print('3rd Step: Printing the losses and saving the real images and the generated images of the minibatch every 100 steps')